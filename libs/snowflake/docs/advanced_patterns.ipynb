{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced LangChain Patterns\n",
    "\n",
    "This notebook demonstrates advanced LangChain patterns that go beyond basic usage, positioning you as a thought leader in LangChain + Snowflake integration.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Router Patterns** - LLM-based decision making and workflow routing\n",
    "2. **Tool-Calling Agents** - Compare LangChain vs LangGraph approaches for intelligent tool usage\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed `getting_started.ipynb` and `snowflake_workflows.ipynb`\n",
    "- Understanding of LangChain basics (chat models, tools, chains)\n",
    "- (Optional) Familiarity with LangGraph concepts\n",
    "\n",
    "## Why These Patterns Matter\n",
    "\n",
    "These patterns represent the cutting edge of LangChain application development:\n",
    "- **Router patterns** enable dynamic workflow selection based on user intent\n",
    "- **Tool-calling agents** provide intelligent, multi-step problem solving\n",
    "- **Framework comparisons** help you choose the right approach for your use case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Router Patterns\n",
    "\n",
    "Router patterns allow LLMs to make intelligent decisions about which workflow, tool, or processing path to take based on user input. This enables dynamic, context-aware applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain_snowflake import ChatSnowflake, create_session_from_env\n",
    "from langchain_snowflake.tools import (\n",
    "    CortexSentimentTool,\n",
    "    CortexSummarizerTool,\n",
    "    SnowflakeCortexAnalyst,\n",
    ")\n",
    "\n",
    "# Initialize session and LLM - consistent with other notebooks\n",
    "session = create_session_from_env()\n",
    "llm = ChatSnowflake(\n",
    "    session=session, model=\"claude-3-5-sonnet\", temperature=0.1, max_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"Session and LLM initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic Router - Workflow Selection\n",
    "\n",
    "The simplest router pattern: decide between different workflows based on user intent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowRoute(BaseModel):\n",
    "    \"\"\"Route for workflow selection.\"\"\"\n",
    "\n",
    "    workflow: Literal[\"analytics\", \"support\", \"content\", \"research\"]\n",
    "    confidence: float\n",
    "    reasoning: str\n",
    "    suggested_action: str\n",
    "\n",
    "\n",
    "def create_workflow_router():\n",
    "    \"\"\"Create a workflow router using structured output.\"\"\"\n",
    "\n",
    "    # Use global session if available, otherwise create fresh one\n",
    "    global session\n",
    "    if session is None:\n",
    "        print(\"Creating fresh session for workflow router...\")\n",
    "        router_session = create_session_from_env()\n",
    "    else:\n",
    "        print(\"Using existing session for workflow router...\")\n",
    "        router_session = session\n",
    "\n",
    "    router_llm_base = ChatSnowflake(\n",
    "        session=router_session,\n",
    "        model=\"claude-3-5-sonnet\",\n",
    "        temperature=0.1,\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    router_llm = router_llm_base.with_structured_output(WorkflowRoute)\n",
    "    print(\"Router created successfully\")\n",
    "\n",
    "    def route_workflow(user_input: str) -> WorkflowRoute:\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the user request and route to the appropriate workflow:\n",
    "        \n",
    "        User Request: \"{user_input}\"\n",
    "        \n",
    "        Available Workflows:\n",
    "        - analytics: Data analysis, SQL queries, business intelligence, metrics\n",
    "        - support: Help requests, troubleshooting, how-to questions  \n",
    "        - content: Text processing, summarization, translation, writing\n",
    "        - research: Information gathering, market research, competitive analysis\n",
    "        \n",
    "        Consider the user's intent and recommend the best workflow with high confidence.\n",
    "        \"\"\"\n",
    "\n",
    "        return router_llm.invoke(prompt)\n",
    "\n",
    "    return route_workflow\n",
    "\n",
    "\n",
    "# Test the workflow router\n",
    "router = create_workflow_router()\n",
    "\n",
    "test_requests = [\n",
    "    \"Show me Q4 sales performance by region\",\n",
    "    \"How do I reset my password?\",\n",
    "    \"Summarize this 10-page document for me\",\n",
    "    \"What are the latest trends in cloud computing?\",\n",
    "    \"Compare sentiment analysis between our product reviews and competitors\",\n",
    "]\n",
    "\n",
    "print(\"\\nWorkflow Router Testing\")\n",
    "\n",
    "for i, request in enumerate(test_requests, 1):\n",
    "    route = router(request)\n",
    "    print(f'\\nRequest {i}: \"{request}\"')\n",
    "\n",
    "    # Handle both Pydantic model and dict responses\n",
    "    if isinstance(route, dict):\n",
    "        # Fallback: structured output returned as dict\n",
    "        workflow = route.get(\"workflow\", \"unknown\")\n",
    "        confidence = route.get(\"confidence\", 0.0)\n",
    "        reasoning = route.get(\"reasoning\", \"No reasoning provided\")\n",
    "        action = route.get(\"suggested_action\", \"No action provided\")\n",
    "    else:\n",
    "        # Expected: Pydantic model with attributes\n",
    "        workflow = route.workflow\n",
    "        confidence = route.confidence\n",
    "        reasoning = route.reasoning\n",
    "        action = route.suggested_action\n",
    "\n",
    "    print(f\"Route: {workflow} (confidence: {confidence:.2f})\")\n",
    "    print(f\"Reasoning: {reasoning}\")\n",
    "    print(f\"Action: {action}\")\n",
    "\n",
    "print(\"Basic workflow routing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Advanced Router - Conditional Execution\n",
    "\n",
    "Execute different processing chains based on routing decisions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_snowflake import ChatSnowflake, create_session_from_env\n",
    "\n",
    "\n",
    "def create_execution_router():\n",
    "    \"\"\"Create a router that executes different chains based on workflow type.\"\"\"\n",
    "\n",
    "    # Use global session if available, otherwise create fresh one\n",
    "    global session\n",
    "    if session is None:\n",
    "        print(\"Creating fresh session for execution router...\")\n",
    "        exec_session = create_session_from_env()\n",
    "    else:\n",
    "        print(\"Using existing session for execution router...\")\n",
    "        exec_session = session\n",
    "\n",
    "    exec_llm = ChatSnowflake(\n",
    "        session=exec_session,\n",
    "        model=\"claude-3-5-sonnet\",\n",
    "        temperature=0.1,\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    print(\"Execution router session ready\")\n",
    "\n",
    "    # Initialize tools with session\n",
    "    sentiment_tool = CortexSentimentTool(session=exec_session)\n",
    "    summarizer_tool = CortexSummarizerTool(session=exec_session)\n",
    "    analyst_tool = SnowflakeCortexAnalyst(session=exec_session)\n",
    "\n",
    "    def analytics_chain(request: str) -> str:\n",
    "        \"\"\"Handle analytics requests.\"\"\"\n",
    "        try:\n",
    "            # Generate SQL with Cortex Analyst\n",
    "            sql_result = analyst_tool.run(request)\n",
    "\n",
    "            # Analyze the result with LLM\n",
    "            analysis_prompt = f\"\"\"\n",
    "            Analyze this SQL generation result for the request: \"{request}\"\n",
    "            \n",
    "            SQL Result: {sql_result}\n",
    "            \n",
    "            Provide insights about what this query will accomplish and any recommendations.\n",
    "            \"\"\"\n",
    "\n",
    "            analysis = exec_llm.invoke(analysis_prompt)\n",
    "            return f\"Analytics Result:\\n{sql_result}\\n\\nAnalysis:\\n{analysis.content}\"\n",
    "        except Exception as e:\n",
    "            return f\"Analytics error: {e}\"\n",
    "\n",
    "    def content_chain(request: str) -> str:\n",
    "        \"\"\"Handle content processing requests.\"\"\"\n",
    "        try:\n",
    "            # First, determine content operation\n",
    "            if \"sentiment\" in request.lower():\n",
    "                # Extract text for sentiment analysis\n",
    "                text_prompt = (\n",
    "                    f\"Extract the main text content from this request: {request}\"\n",
    "                )\n",
    "                text_response = exec_llm.invoke(text_prompt)\n",
    "                text = text_response.content\n",
    "\n",
    "                sentiment_result = sentiment_tool.run(text)\n",
    "                return f\"Sentiment Analysis:\\n{sentiment_result}\"\n",
    "\n",
    "            elif \"summary\" in request.lower() or \"summarize\" in request.lower():\n",
    "                # Extract text for summarization\n",
    "                text_prompt = (\n",
    "                    f\"Extract the main text content to summarize from: {request}\"\n",
    "                )\n",
    "                text_response = exec_llm.invoke(text_prompt)\n",
    "                text = text_response.content\n",
    "\n",
    "                summary_result = summarizer_tool.run(text)\n",
    "                return f\"Summary:\\n{summary_result}\"\n",
    "\n",
    "            else:\n",
    "                # General content processing\n",
    "                content_prompt = f\"\"\"\n",
    "                Process this content request: \"{request}\"\n",
    "                \n",
    "                Provide appropriate text processing, writing assistance, or content guidance.\n",
    "                \"\"\"\n",
    "                content_response = exec_llm.invoke(content_prompt)\n",
    "                return f\"Content Processing:\\n{content_response.content}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Content processing error: {e}\"\n",
    "\n",
    "    def support_chain(request: str) -> str:\n",
    "        \"\"\"Handle support requests.\"\"\"\n",
    "        support_prompt = f\"\"\"\n",
    "        Provide helpful support for this request: \"{request}\"\n",
    "        \n",
    "        Give clear, actionable guidance and troubleshooting steps if applicable.\n",
    "        Focus on being helpful and solving the user's problem.\n",
    "        \"\"\"\n",
    "\n",
    "        support_response = exec_llm.invoke(support_prompt)\n",
    "        return f\"Support Response:\\n{support_response.content}\"\n",
    "\n",
    "    def research_chain(request: str) -> str:\n",
    "        \"\"\"Handle research requests.\"\"\"\n",
    "        research_prompt = f\"\"\"\n",
    "        Provide comprehensive research insights for: \"{request}\"\n",
    "        \n",
    "        Include relevant background, current trends, key considerations, and actionable recommendations.\n",
    "        Structure your response as a brief research report.\n",
    "        \"\"\"\n",
    "\n",
    "        research_response = exec_llm.invoke(research_prompt)\n",
    "        return f\"Research Report:\\n{research_response.content}\"\n",
    "\n",
    "    # Router execution function\n",
    "    def execute_request(request: str) -> str:\n",
    "        \"\"\"Route and execute request based on workflow type.\"\"\"\n",
    "\n",
    "        # First, route the request\n",
    "        route = router(request)\n",
    "\n",
    "        # Handle both Pydantic model and dict responses\n",
    "        if isinstance(route, dict):\n",
    "            workflow = route.get(\"workflow\", \"unknown\")\n",
    "        else:\n",
    "            workflow = route.workflow\n",
    "\n",
    "        print(f\"Routing '{request[:50]}...' to '{workflow}' workflow\")\n",
    "\n",
    "        # Execute appropriate chain based on route\n",
    "        if workflow == \"analytics\":\n",
    "            return analytics_chain(request)\n",
    "        elif workflow == \"content\":\n",
    "            return content_chain(request)\n",
    "        elif workflow == \"support\":\n",
    "            return support_chain(request)\n",
    "        elif workflow == \"research\":\n",
    "            return research_chain(request)\n",
    "        else:\n",
    "            return f\"Unknown workflow: {workflow}\"\n",
    "\n",
    "    return execute_request\n",
    "\n",
    "\n",
    "# Test the execution router\n",
    "execution_router = create_execution_router()\n",
    "\n",
    "test_executions = [\n",
    "    \"Show me top customers by revenue this quarter\",\n",
    "    \"Analyze the sentiment of: 'This new feature is absolutely fantastic!'\",\n",
    "    \"How do I configure SSL settings in Snowflake?\",\n",
    "    \"What are the emerging trends in real-time data processing?\",\n",
    "]\n",
    "\n",
    "print(\"\\nExecution Router Testing\")\n",
    "\n",
    "for i, request in enumerate(test_executions, 1):\n",
    "    print(f\"Request {i}: {request}\")\n",
    "\n",
    "    try:\n",
    "        result = execution_router(request)\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Execution error: {e}\")\n",
    "\n",
    "print(\"\\nAdvanced conditional routing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tool-Calling Agents\n",
    "\n",
    "Compare LangChain vs LangGraph approaches for intelligent tool usage and multi-step reasoning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 LangChain Approach - bind_tools with multi-tool planning\n",
    "\n",
    "The LangChain approach uses `bind_tools` with manual execution for better multi-tool coordination:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from datetime import datetime\n",
    "from langchain_snowflake import (\n",
    "    CortexSentimentTool,\n",
    "    CortexSummarizerTool,\n",
    "    CortexTranslatorTool,\n",
    "    SnowflakeCortexAnalyst,\n",
    "    SnowflakeQueryTool,\n",
    ")\n",
    "\n",
    "@tool\n",
    "def get_current_timestamp() -> str:\n",
    "    \"\"\"Get the current timestamp in a readable format.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "@tool  \n",
    "def calculate_percentage_change(old_value: float, new_value: float) -> str:\n",
    "    \"\"\"Calculate percentage change between two values.\"\"\"\n",
    "    if old_value == 0:\n",
    "        return \"Cannot calculate percentage change from zero\"\n",
    "    \n",
    "    change = ((new_value - old_value) / old_value) * 100\n",
    "    return f\"Change from {old_value} to {new_value}: {change:.2f}% {'increase' if change > 0 else 'decrease'}\"\n",
    "\n",
    "class LangChainToolAgent:\n",
    "    \"\"\"Tool-calling agent using LangChain's native bind_tools.\"\"\"\n",
    "\n",
    "    def __init__(self, session):\n",
    "        self.session = session\n",
    "\n",
    "        # Initialize Snowflake tools\n",
    "        self.snowflake_tools = [\n",
    "            CortexSentimentTool(session=session),\n",
    "            CortexSummarizerTool(session=session),\n",
    "            CortexTranslatorTool(session=session),\n",
    "            SnowflakeCortexAnalyst(session=session),\n",
    "            SnowflakeQueryTool(session=session),\n",
    "        ]\n",
    "\n",
    "        # General tools\n",
    "        self.general_tools = [get_current_timestamp, calculate_percentage_change]\n",
    "\n",
    "        # Combine all tools\n",
    "        self.all_tools = self.snowflake_tools + self.general_tools\n",
    "\n",
    "        # Create LLM with bound tools\n",
    "        self.llm = ChatSnowflake(\n",
    "            session=session, model=\"claude-4-sonnet\", temperature=0.1, max_tokens=1500\n",
    "        ).bind_tools(self.all_tools)\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        \"\"\"Execute query using pure native LLM tool selection.\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke(query)\n",
    "\n",
    "            if hasattr(response, \"tool_calls\") and response.tool_calls:\n",
    "                tool_count = len(response.tool_calls)\n",
    "                print(f\"   LLM selected {tool_count} tool(s)\")\n",
    "                return self._execute_tool_calls(response)\n",
    "            else:\n",
    "                print(f\"   No tools called - LLM provided direct response\")\n",
    "                return response.content\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    def _execute_tool_calls(self, response) -> str:\n",
    "        \"\"\"Execute the tools selected by the LLM.\"\"\"\n",
    "        tool_results = []\n",
    "        \n",
    "        for tool_call in response.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "\n",
    "            # Find and execute the tool\n",
    "            for tool in self.all_tools:\n",
    "                if tool.name == tool_name:\n",
    "                    try:\n",
    "                        result = tool.invoke(tool_args)\n",
    "                        tool_results.append(f\"• {tool_name}: {result}\")\n",
    "                        print(f\"      Executed {tool_name}\")\n",
    "                    except Exception as e:\n",
    "                        tool_results.append(f\"• {tool_name}: Error - {e}\")\n",
    "                        print(f\"      Error in {tool_name}: {e}\")\n",
    "                    break\n",
    "\n",
    "        # Return results\n",
    "        if tool_results:\n",
    "            return response.content + \"\\n\\nTool Execution Results:\\n\" + \"\\n\".join(tool_results)\n",
    "        else:\n",
    "            return response.content\n",
    "\n",
    "# Create and test the agent\n",
    "langchain_agent = LangChainToolAgent(session)\n",
    "\n",
    "# Test queries that FORCE tool usage (these cannot be answered without tools)\n",
    "langchain_test_queries = [\n",
    "    \"What time it is right now?\",\n",
    "    \"find the percentage change from 100 to 150\",\n",
    "    \"Analyze the sentiment of this text: 'I love this product!'\",\n",
    "    \"Please translate 'Hello world' to Spanish and French\",\n",
    "    \"Execute this query: SELECT 'Tools working!' as status, CURRENT_TIMESTAMP() as timestamp, Tell me the current timestamp and then do a sentiment analysis of: 'I like this movie!'\",\n",
    "]\n",
    "\n",
    "print(f\"Available tools: {len(langchain_agent.all_tools)} total\")\n",
    "print(\"Tool List:\")\n",
    "for tool in langchain_agent.all_tools:\n",
    "    print(f\"   - {tool.name}: {tool.description}\")\n",
    "\n",
    "for i, query in enumerate(langchain_test_queries, 1):\n",
    "    print(f\"Test {i}: {query}\")\n",
    "    \n",
    "    try:\n",
    "        result = langchain_agent.run(query)\n",
    "        print(f\"Response: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\nLangChain approach completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 LangGraph Approach - ToolNode with Multi-Step Reasoning\n",
    "\n",
    "The LangGraph approach uses explicit state management and ToolNode for more complex workflows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langchain_core.messages import AIMessage, HumanMessage\n",
    "    from langgraph.graph import END, StateGraph\n",
    "    from langgraph.graph.message import MessagesState\n",
    "    from langgraph.prebuilt import ToolNode\n",
    "\n",
    "    from langchain_snowflake import (\n",
    "        CortexSentimentTool,\n",
    "        CortexSummarizerTool,\n",
    "        CortexTranslatorTool,\n",
    "        SnowflakeCortexAnalyst,\n",
    "        SnowflakeQueryTool,\n",
    "    )\n",
    "\n",
    "    LANGGRAPH_AVAILABLE = True\n",
    "    print(\"LangGraph available\")\n",
    "except ImportError:\n",
    "    LANGGRAPH_AVAILABLE = False\n",
    "    print(\"LangGraph not available - install with: pip install langgraph\")\n",
    "\n",
    "if LANGGRAPH_AVAILABLE:\n",
    "\n",
    "    class LangGraphToolAgent:\n",
    "        \"\"\"Tool-calling agent using LangGraph's ToolNode approach.\"\"\"\n",
    "\n",
    "        def __init__(self, session):\n",
    "            self.session = session\n",
    "\n",
    "            # Initialize tools (same as LangChain)\n",
    "            self.snowflake_tools = [\n",
    "                CortexSentimentTool(session=session),\n",
    "                CortexSummarizerTool(session=session),\n",
    "                CortexTranslatorTool(session=session),\n",
    "                SnowflakeCortexAnalyst(session=session),\n",
    "                SnowflakeQueryTool(session=session),\n",
    "            ]\n",
    "\n",
    "            self.general_tools = [get_current_timestamp, calculate_percentage_change]\n",
    "\n",
    "            self.all_tools = self.snowflake_tools + self.general_tools\n",
    "\n",
    "            # Create LLM with tools (no auto-execute - LangGraph handles this)\n",
    "            self.llm = ChatSnowflake(\n",
    "                session=session,\n",
    "                model=\"claude-3-5-sonnet\",\n",
    "                temperature=0.1,\n",
    "                max_tokens=1500,\n",
    "            ).bind_tools(self.all_tools)\n",
    "\n",
    "            # Build LangGraph workflow\n",
    "            self.workflow = self._build_workflow()\n",
    "\n",
    "        def _build_workflow(self):\n",
    "            \"\"\"Build the LangGraph workflow.\"\"\"\n",
    "\n",
    "            # Create tool node for executing tools\n",
    "            tool_node = ToolNode(self.all_tools)\n",
    "\n",
    "            def call_model(state: MessagesState):\n",
    "                \"\"\"Call the model to get tool calls or final response.\"\"\"\n",
    "                messages = state[\"messages\"]\n",
    "                response = self.llm.invoke(messages)\n",
    "                return {\"messages\": [response]}\n",
    "\n",
    "            def should_continue(state: MessagesState):\n",
    "                \"\"\"Decide whether to continue with tools or end.\"\"\"\n",
    "                messages = state[\"messages\"]\n",
    "                last_message = messages[-1]\n",
    "\n",
    "                # If the last message has tool calls, continue to tools\n",
    "                if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "                    return \"tools\"\n",
    "                # Otherwise, end the conversation\n",
    "                return END\n",
    "\n",
    "            # Build the graph\n",
    "            workflow = StateGraph(MessagesState)\n",
    "\n",
    "            # Add nodes\n",
    "            workflow.add_node(\"agent\", call_model)\n",
    "            workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "            # Add edges\n",
    "            workflow.set_entry_point(\"agent\")\n",
    "            workflow.add_conditional_edges(\n",
    "                \"agent\", should_continue, {\"tools\": \"tools\", END: END}\n",
    "            )\n",
    "            workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "            return workflow.compile()\n",
    "\n",
    "        def run(self, query: str) -> str:\n",
    "            \"\"\"Execute query using LangGraph workflow.\"\"\"\n",
    "            try:\n",
    "                initial_state = {\"messages\": [HumanMessage(content=query)]}\n",
    "\n",
    "                # Run the workflow\n",
    "                result = self.workflow.invoke(initial_state)\n",
    "\n",
    "                # Get the final response\n",
    "                final_message = result[\"messages\"][-1]\n",
    "                return final_message.content\n",
    "\n",
    "            except Exception as e:\n",
    "                return f\"LangGraph agent error: {e}\"\n",
    "\n",
    "        async def arun(self, query: str) -> str:\n",
    "            \"\"\"Execute query asynchronously using LangGraph.\"\"\"\n",
    "            try:\n",
    "                initial_state = {\"messages\": [HumanMessage(content=query)]}\n",
    "\n",
    "                # Run the workflow asynchronously\n",
    "                result = await self.workflow.ainvoke(initial_state)\n",
    "\n",
    "                # Get the final response\n",
    "                final_message = result[\"messages\"][-1]\n",
    "                return final_message.content\n",
    "\n",
    "            except Exception as e:\n",
    "                return f\"LangGraph async agent error: {e}\"\n",
    "\n",
    "    # Test LangGraph approach\n",
    "    print(\"\\nLangGraph Tool-Calling Agent\")\n",
    "\n",
    "    langgraph_agent = LangGraphToolAgent(session)\n",
    "\n",
    "    langgraph_test_queries = [\n",
    "        \"What's the current timestamp and analyze the sentiment of: 'Amazing results!'\",\n",
    "        \"Calculate percentage change from 200 to 250, then summarize what this means\",\n",
    "        \"Create SQL query for customer sales grouped by product and get the current time\",\n",
    "    ]\n",
    "\n",
    "    print(f\"🛠️ Available tools: {len(langgraph_agent.all_tools)} total\")\n",
    "    print(\"   • 5 Snowflake Cortex tools\")\n",
    "    print(\"   • 2 general-purpose tools\")\n",
    "    print(\"   • Explicit multi-step reasoning\")\n",
    "    print(\"   • State management with MessagesState\")\n",
    "\n",
    "    for i, query in enumerate(langgraph_test_queries, 1):\n",
    "        print(f\"LangGraph Query {i}: {query}\")\n",
    "\n",
    "        try:\n",
    "            result = langgraph_agent.run(query)\n",
    "            print(f\"Response: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    print(\"\\nLangGraph approach completed!\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"\\nLangGraph section skipped - please install langgraph to test this approach\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've mastered advanced LangChain patterns with Snowflake:\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "1. **Router Patterns**\n",
    "   - Built intelligent workflow routing based on user intent\n",
    "   - Implemented conditional execution chains\n",
    "   - Created dynamic, context-aware applications\n",
    "\n",
    "2. **Tool-Calling Agents**\n",
    "   - Compared LangChain vs LangGraph approaches\n",
    "   - Built multi-tool coordination systems\n",
    "   - Understood when to use each framework\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Production Deployment**\n",
    "   - Implement proper error handling and retries\n",
    "   - Add monitoring and logging for tool executions\n",
    "   - Set up authentication and security\n",
    "\n",
    "2. **Advanced Patterns**\n",
    "   - Explore LangGraph's advanced features (subgraphs, human-in-the-loop)\n",
    "   - Build multi-agent systems with state sharing\n",
    "   - Implement custom tool execution strategies\n",
    "\n",
    "3. **Snowflake Integration**\n",
    "   - Create custom tools for your specific Snowflake data\n",
    "   - Build semantic models for better Cortex Analyst results\n",
    "   - Implement RAG with your organization's data\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Router patterns** enable intelligent application behavior\n",
    "- **LangChain bind_tools** is perfect for simple, fast tool calling\n",
    "- **LangGraph ToolNode** excels at complex, multi-step workflows\n",
    "- **Snowflake Cortex** provides powerful AI capabilities for data-driven applications\n",
    "\n",
    "**You're now equipped to build sophisticated LangChain applications with Snowflake!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-snowflake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
